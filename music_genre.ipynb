{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99ea4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from tempfile import TemporaryFile\n",
    "import os\n",
    "import pickle\n",
    "import random \n",
    "import operator\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353371b",
   "metadata": {},
   "source": [
    "The first step in any automatic speech recognition system is to extract features i.e. identify the components of the audio signal that are good for identifying the linguistic content and discarding all the other stuff which carries information like background noise, emotion etc.\n",
    "\n",
    "The main point to understand about speech is that the sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffa3d5",
   "metadata": {},
   "source": [
    "Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in automatic speech and speaker recognition. They were introduced by Davis and Mermelstein in the 1980's, and have been state-of-the-art ever since. STEPS:\n",
    "- Frame the signal into short frames.\n",
    "- For each frame calculate the periodogram estimate of the power spectrum.\n",
    "- Apply the mel filterbank to the power spectra, sum the energy in each filter.\n",
    "- Take the logarithm of all filterbank energies.\n",
    "- Take the DCT of the log filterbank energies.\n",
    "- Keep DCT coefficients 2-13, discard the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b30f8",
   "metadata": {},
   "source": [
    "An audio signal is constantly changing, so to simplify things we assume that on short time scales the audio signal doesn't change much (when we say it doesn't change, we mean statistically i.e. statistically stationary, obviously the samples are constantly changing on even short time scales). This is why we frame the signal into 20-40ms frames. If the frame is much shorter we don't have enough samples to get a reliable spectral estimate, if it is longer the signal changes too much throughout the frame.\n",
    "\n",
    "The next step is to calculate the power spectrum of each frame. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame.\n",
    "\n",
    "The periodogram spectral estimate still contains a lot of information not required for Automatic Speech Recognition (ASR). In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions. This is performed by our Mel filterbank: the first filter is very narrow and gives an indication of how much energy exists near 0 Hertz. As the frequencies get higher our filters get wider as we become less concerned about variations. We are only interested in roughly how much energy occurs at each spot. The Mel scale tells us exactly how to space our filterbanks and how wide to make them. See below for how to calculate the spacing.\n",
    "\n",
    "Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. Why the logarithm and not a cube root? The logarithm allows us to use cepstral mean subtraction, which is a channel normalisation technique.\n",
    "\n",
    "The final step is to compute the DCT of the log filterbank energies. There are 2 main reasons this is performed. Because our filterbanks are all overlapping, the filterbank energies are quite correlated with each other. The DCT decorrelates the energies which means diagonal covariance matrices can be used to model the features in e.g. a HMM classifier. But notice that only 12 of the 26 DCT coefficients are kept. This is because the higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568417e",
   "metadata": {},
   "source": [
    "### 2. Define a function to get the distance between feature vectors and find neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62df4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbors(trainingSet, instance, k):\n",
    "    distances = []\n",
    "    for x in range (len(trainingSet)):\n",
    "        dist = distance(trainingSet[x], instance, k )+ distance(instance, trainingSet[x], k)\n",
    "        distances.append((trainingSet[x][2], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90699c",
   "metadata": {},
   "source": [
    "### 3. Identify the nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62347d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearestClass(neighbors):\n",
    "    classVote = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x]\n",
    "        if response in classVote:\n",
    "            classVote[response]+=1 \n",
    "        else:\n",
    "            classVote[response]=1\n",
    "    sorter = sorted(classVote.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    return sorter[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f54534",
   "metadata": {},
   "source": [
    "###  4. Define a function for model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab89bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0 \n",
    "    for x in range (len(testSet)):\n",
    "        if testSet[x][-1]==predictions[x]:\n",
    "            correct+=1\n",
    "    return 1.0*correct/len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec73fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(instance1 , instance2 , k ):\n",
    "    distance =0 \n",
    "    mm1 = instance1[0] \n",
    "    cm1 = instance1[1]\n",
    "    mm2 = instance2[0]\n",
    "    cm2 = instance2[1]\n",
    "    distance = np.trace(np.dot(np.linalg.inv(cm2), cm1)) \n",
    "    distance+=(np.dot(np.dot((mm2-mm1).transpose() , np.linalg.inv(cm2)) , mm2-mm1 )) \n",
    "    distance+= np.log(np.linalg.det(cm2)) - np.log(np.linalg.det(cm1))\n",
    "    distance-= k\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2979f84",
   "metadata": {},
   "source": [
    "### 5. Extract features from the dataset and dump these features into a binary .dat file “my.dat”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we obtain the actual directory of where the file is located\n",
    "directory = os.getcwd()+'/genres/'\n",
    "\n",
    "#We create my.dat that will be for writing in binary\n",
    "f= open(\"my.dat\" ,'wb')\n",
    "i=0\n",
    "#We visit each folder of our data directory\n",
    "for folder in os.listdir(directory):\n",
    "    #We check that only visit folders\n",
    "    i+=1\n",
    "    if \".mf\" not in folder: \n",
    "        #we visist each file of the folder\n",
    "        for file in os.listdir(directory+\"/\"+folder):  \n",
    "            #We read the wav file(music)\n",
    "            (rate,sig) = wav.read(directory+folder+\"/\"+file)\n",
    "            \n",
    "            mfcc_feat = mfcc(sig,rate ,winlen=0.020, appendEnergy = False)\n",
    "            \n",
    "            covariance = np.cov(np.matrix.transpose(mfcc_feat))\n",
    "            \n",
    "            mean_matrix = mfcc_feat.mean(0)\n",
    "            \n",
    "            feature = (mean_matrix , covariance,i)\n",
    "            \n",
    "            pickle.dump(feature , f)\n",
    "            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8293de",
   "metadata": {},
   "source": [
    "### 6. Train and test split on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eea3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "def loadDataset(filename , split , trSet , teSet):\n",
    "    with open(\"my.dat\" , 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                dataset.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                f.close()\n",
    "                break  \n",
    "    for x in range(len(dataset)):\n",
    "        if random.random() <split :      \n",
    "            trSet.append(dataset[x])\n",
    "        else:\n",
    "            teSet.append(dataset[x])  \n",
    "trainingSet = []\n",
    "testSet = []\n",
    "loadDataset(\"my.dat\" , 0.66, trainingSet, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046be5ef",
   "metadata": {},
   "source": [
    "### 7. Make prediction using KNN and get the accuracy on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2919629",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e73250cfe6af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mleng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnearestClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0maccuracy1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-48fe0b099e11>\u001b[0m in \u001b[0;36mgetNeighbors\u001b[1;34m(trainingSet, instance, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mneighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "leng = len(testSet)\n",
    "predictions = []\n",
    "for x in range (leng):\n",
    "    predictions.append(nearestClass(getNeighbors(trainingSet ,testSet[x] , 5))) \n",
    "accuracy1 = getAccuracy(testSet , predictions)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cdc4d",
   "metadata": {},
   "source": [
    "###  Test with new Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c090b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = []\n",
    "def loadDataset(filename):\n",
    "    with open(\"my.dat\" , 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                dataset.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                f.close()\n",
    "                break\n",
    "loadDataset(\"my.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106941d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "for folder in os.listdir(\"./musics/wav_genres/\"):\n",
    "    results[i]=folder\n",
    "    i+=1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(rate,sig)=wav.read(\"__path_to_new_audio_file_\")\n",
    "mfcc_feat=mfcc(sig,rate,winlen=0.020,appendEnergy=False)\n",
    "covariance = np.cov(np.matrix.transpose(mfcc_feat))\n",
    "mean_matrix = mfcc_feat.mean(0)\n",
    "feature=(mean_matrix,covariance,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=nearestClass(getNeighbors(dataset ,feature , 5))\n",
    "print(results[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96055acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
